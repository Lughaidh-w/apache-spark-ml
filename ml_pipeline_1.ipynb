{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2d141d-f655-4296-ad45-55df2fb5ea09",
   "metadata": {},
   "source": [
    "## ML Pipeline in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d050f5-901b-4b56-9f26-ef162c83594c",
   "metadata": {},
   "source": [
    "Performs ETL and ML on MPG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5421a739-2db4-4d6e-898f-3dcfbe9f92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05a5cc-9320-4cee-a370-fe93244a1bbf",
   "metadata": {},
   "source": [
    "### Part 1: ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4102927d-63a5-4044-b139-c5b4c6f1dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 15:24:21 WARN Utils: Your hostname, ubuntu-MS-7D15 resolves to a loopback address: 127.0.1.1; using 192.168.1.3 instead (on interface enp5s0)\n",
      "24/06/10 15:24:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/miniconda3/envs/cct/lib/python3.11/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/06/10 15:24:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MPG_Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2630e5e1-c398-4a12-88fd-b518775c6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db85dfb-e177-4296-8aa7-fdb7016bbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"mpg-raw.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae241de-2034-4d6d-aefd-c4fbf13c8bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "|46.6|        4|       86.0|        65|  2110|      17.9|  80|Japanese|\n",
      "|44.6|        4|       91.0|        67|  1850|      13.8|  80|Japanese|\n",
      "|44.3|        4|       90.0|        48|  2085|      21.7|  80|European|\n",
      "|44.0|        4|       97.0|        52|  2130|      24.6|  82|European|\n",
      "|43.4|        4|       90.0|        48|  2335|      23.7|  80|European|\n",
      "+----+---------+-----------+----------+------+----------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2955917-1ded-4ac0-85ab-e81055a72947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Engine Disp\",\"Engine_Disp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d08440e-c6e3-409e-a28f-b60a8c58a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Origin|count|\n",
      "+--------+-----+\n",
      "|    null|    1|\n",
      "|European|   70|\n",
      "|Japanese|   88|\n",
      "|American|  247|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Origin').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c79d4-d088-4469-bac1-7862874bfa38",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deba3b2a-5584-4342-a304-dc5ad38e5899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n"
     ]
    }
   ],
   "source": [
    "rowcount1 = df.count()\n",
    "print(rowcount1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df878d3-82d4-459c-9724-be7a5287f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28568544-1cce-4421-ac1b-928b64d91679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "rowcount2 = df.count()\n",
    "print(rowcount2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726f4b0-bced-4cc0-9bef-f14c0e218daf",
   "metadata": {},
   "source": [
    "#### drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f575bec1-2d6c-4c2c-9b2e-303dba4bec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e176bd5-2e93-476d-ab3a-ffcc3b52c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "rowcount3 = df.count()\n",
    "print(rowcount3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b00f8b8-75db-46da-9a64-97eb2a3e4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/06/10 15:24:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"mpg-cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cedb51f9-07d4-4359-a8c9-3594b632abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 - Evaluation\n",
      "Total rows =  406\n",
      "Total rows after dropping duplicate rows =  392\n",
      "Total rows after dropping duplicate rows and rows with null values =  385\n",
      "Renamed column name =  Engine_Disp\n",
      "mpg-cleaned.parquet exists : True\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 1 - Evaluation\")\n",
    "\n",
    "print(\"Total rows = \", rowcount1)\n",
    "print(\"Total rows after dropping duplicate rows = \", rowcount2)\n",
    "print(\"Total rows after dropping duplicate rows and rows with null values = \", rowcount3)\n",
    "print(\"Renamed column name = \", df.columns[2])\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"mpg-cleaned.parquet exists :\", os.path.isdir(\"mpg-cleaned.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb483d62-8b1d-42fc-8cc6-376caa90d9fb",
   "metadata": {},
   "source": [
    "### Part 2: ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f81c21f0-f104-4f84-8063-b0f146de5eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"mpg-cleaned.parquet\")\n",
    "rowcount4 = df.count()\n",
    "print(rowcount4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664305bf-0f8d-464d-8c20-8a8d81595df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"Origin\", outputCol=\"OriginIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73eafcc9-5529-4801-8f96-c78aaba20fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Cylinders','Engine_Disp','Horsepower','Weight','Accelerate','Year'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b656d8-35c7-4252-ae7c-2fab3b65dd0d",
   "metadata": {},
   "source": [
    "#### standard scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29fec4ff-f5e3-44f7-91a2-eede1bca7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d357888-f61e-48ba-bc2b-44e416658dbb",
   "metadata": {},
   "source": [
    "#### lr pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2df9834-8f3b-4bbb-b6d5-f3bf708601f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"MPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1ad76f-4ecb-492b-a92b-cbd4a4f5545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer,assembler, scaler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f136b-bc0d-46aa-acf4-e5c84f4bd16e",
   "metadata": {},
   "source": [
    "#### train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5cf0b47-0ee8-48c5-a72a-f690e97056bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9d061-bdd5-4f1b-bc64-85a489b292de",
   "metadata": {},
   "source": [
    "#### pipeline fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f640afb-0412-4a53-a98f-2be7df38b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 15:24:43 WARN Instrumentation: [aaddc371] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/06/10 15:24:43 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/06/10 15:24:43 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/06/10 15:24:43 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "24/06/10 15:24:43 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f59499-9885-4684-9a39-bfb0a53519d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 - Evaluation\n",
      "Total rows =  385\n",
      "Pipeline Stage 1 =  StringIndexer\n",
      "Pipeline Stage 2 =  VectorAssembler\n",
      "Pipeline Stage 3 =  StandardScaler\n",
      "Label column =  MPG\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 2 - Evaluation\")\n",
    "print(\"Total rows = \", rowcount4)\n",
    "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
    "\n",
    "print(\"Pipeline Stage 1 = \", ps[0])\n",
    "print(\"Pipeline Stage 2 = \", ps[1])\n",
    "print(\"Pipeline Stage 3 = \", ps[2])\n",
    "\n",
    "print(\"Label column = \", lr.getLabelCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df608d-51cd-4d38-b3eb-c12560142c43",
   "metadata": {},
   "source": [
    "### Part 3: model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3b38b66-c0f7-4e98-9337-b6dd64e42c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2f2194-7446-46fe-97fa-29b2d94c398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.279864329372996\n",
      "2.350907028306923\n",
      "0.8170972243874175\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(mse)\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(mae)\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f10f7bf-6777-4b70-95fb-b05e26cde39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 - Evaluation\n",
      "Mean Squared Error =  9.28\n",
      "Mean Absolute Error =  2.35\n",
      "R Squared =  0.82\n",
      "Intercept =  -10.48\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 3 - Evaluation\")\n",
    "\n",
    "print(\"Mean Squared Error = \", round(mse,2))\n",
    "print(\"Mean Absolute Error = \", round(mae,2))\n",
    "print(\"R Squared = \", round(r2,2))\n",
    "\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "\n",
    "print(\"Intercept = \", round(lrModel.intercept,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f10aaae7-9f9f-4107-858b-fef1eed9e44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 4 - Evaluation\n",
      "Number of stages in the pipeline =  4\n",
      "Coefficient for Cylinders is -0.5029\n",
      "Coefficient for Engine_Disp is 0.7078\n",
      "Coefficient for Horsepower is 0.1952\n",
      "Coefficient for Weight is -6.077\n",
      "Coefficient for Accelerate is 0.239\n",
      "Coefficient for Year is 2.6966\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 4 - Evaluation\")\n",
    "\n",
    "loadedmodel = pipelineModel.stages[-1]\n",
    "totalstages = len(pipelineModel.stages)\n",
    "inputcolumns = pipelineModel.stages[1].getInputCols()\n",
    "\n",
    "print(\"Number of stages in the pipeline = \", totalstages)\n",
    "for i,j in zip(inputcolumns, loadedmodel.coefficients):\n",
    "    print(f\"Coefficient for {i} is {round(j,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f6b71-ab26-4518-b8b8-0154ad6a430a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
